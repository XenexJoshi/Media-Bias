{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential modules for the machine-learning process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>heading</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>bias_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Gun Violence Over Fourth of July Weekend</td>\n",
       "      <td>['Protests', 'Fourth Of July', 'Gun Control An...</td>\n",
       "      <td>Chicago Gun Violence Spikes and Increasingly F...</td>\n",
       "      <td>New York Times (News)</td>\n",
       "      <td>As Yasmin Miller drove home from a laundromat ...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gun Violence Over Fourth of July Weekend</td>\n",
       "      <td>['Protests', 'Fourth Of July', 'Gun Control An...</td>\n",
       "      <td>‘Bullets just came from nowhere’: Fourth of Ju...</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>As many Chicagoans were celebrating the Fourth...</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Gun Violence Over Fourth of July Weekend</td>\n",
       "      <td>['Protests', 'Fourth Of July', 'Gun Control An...</td>\n",
       "      <td>Dozens of shootings across US mark bloody July...</td>\n",
       "      <td>New York Post (News)</td>\n",
       "      <td>The nation’s 4th of July weekend was marred by...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Yellen Warns Congress of 'Economic Recession' ...</td>\n",
       "      <td>['Janet Yellen', 'Debt Ceiling', 'Economic Pol...</td>\n",
       "      <td>Federal Government Will Run Out of Cash on Oct...</td>\n",
       "      <td>The Epoch Times</td>\n",
       "      <td>Treasury Secretary Janet Yellen on Tuesday war...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yellen Warns Congress of 'Economic Recession' ...</td>\n",
       "      <td>['Janet Yellen', 'Debt Ceiling', 'Economic Pol...</td>\n",
       "      <td>Yellen tells Congress that U.S. will run out o...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Treasury Secretary Janet Yellen on Tuesday tol...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0           Gun Violence Over Fourth of July Weekend   \n",
       "1   1           Gun Violence Over Fourth of July Weekend   \n",
       "2   2           Gun Violence Over Fourth of July Weekend   \n",
       "3   3  Yellen Warns Congress of 'Economic Recession' ...   \n",
       "4   4  Yellen Warns Congress of 'Economic Recession' ...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['Protests', 'Fourth Of July', 'Gun Control An...   \n",
       "1  ['Protests', 'Fourth Of July', 'Gun Control An...   \n",
       "2  ['Protests', 'Fourth Of July', 'Gun Control An...   \n",
       "3  ['Janet Yellen', 'Debt Ceiling', 'Economic Pol...   \n",
       "4  ['Janet Yellen', 'Debt Ceiling', 'Economic Pol...   \n",
       "\n",
       "                                             heading                 source  \\\n",
       "0  Chicago Gun Violence Spikes and Increasingly F...  New York Times (News)   \n",
       "1  ‘Bullets just came from nowhere’: Fourth of Ju...        Chicago Tribune   \n",
       "2  Dozens of shootings across US mark bloody July...   New York Post (News)   \n",
       "3  Federal Government Will Run Out of Cash on Oct...        The Epoch Times   \n",
       "4  Yellen tells Congress that U.S. will run out o...        Washington Post   \n",
       "\n",
       "                                                text bias_rating  \n",
       "0  As Yasmin Miller drove home from a laundromat ...        left  \n",
       "1  As many Chicagoans were celebrating the Fourth...      center  \n",
       "2  The nation’s 4th of July weekend was marred by...       right  \n",
       "3  Treasury Secretary Janet Yellen on Tuesday war...       right  \n",
       "4  Treasury Secretary Janet Yellen on Tuesday tol...        left  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the source dataset obtained from source: https://github.com/irgroup/Qbias\n",
    "dataset = pd.read_csv(\"bias_data.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>bias_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Gun Violence Over Fourth of July Weekend</td>\n",
       "      <td>As Yasmin Miller drove home from a laundromat ...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gun Violence Over Fourth of July Weekend</td>\n",
       "      <td>As many Chicagoans were celebrating the Fourth...</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Gun Violence Over Fourth of July Weekend</td>\n",
       "      <td>The nation’s 4th of July weekend was marred by...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Yellen Warns Congress of 'Economic Recession' ...</td>\n",
       "      <td>Treasury Secretary Janet Yellen on Tuesday war...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yellen Warns Congress of 'Economic Recession' ...</td>\n",
       "      <td>Treasury Secretary Janet Yellen on Tuesday tol...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0           Gun Violence Over Fourth of July Weekend   \n",
       "1   1           Gun Violence Over Fourth of July Weekend   \n",
       "2   2           Gun Violence Over Fourth of July Weekend   \n",
       "3   3  Yellen Warns Congress of 'Economic Recession' ...   \n",
       "4   4  Yellen Warns Congress of 'Economic Recession' ...   \n",
       "\n",
       "                                                text bias_rating  \n",
       "0  As Yasmin Miller drove home from a laundromat ...        left  \n",
       "1  As many Chicagoans were celebrating the Fourth...      center  \n",
       "2  The nation’s 4th of July weekend was marred by...       right  \n",
       "3  Treasury Secretary Janet Yellen on Tuesday war...       right  \n",
       "4  Treasury Secretary Janet Yellen on Tuesday tol...        left  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the essential data columns for the text-based media bias classification\n",
    "bias_data  = dataset[['id', 'title', 'text', 'bias_rating']]\n",
    "input_text = bias_data['text'].astype(str)\n",
    "input_label = bias_data['bias_rating'].astype(str)\n",
    "bias_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Splitting data in 70-30 training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_text, input_label, test_size = 0.30, random_state = 42)\n",
    "\n",
    "# Generating bag-of-words for Naive-Bayes and SVM, after pre-processing by removing standard English stopwords and converting \n",
    "# text to lowercase\n",
    "vectorizer = CountVectorizer(stop_words = \"english\", lowercase = True)\n",
    "train_vector = vectorizer.fit_transform(x_train)\n",
    "test_vector = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the naive bayes classifier is: 44.599%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training the Multinomial Naive Bayes Model on the vectorized bag-of-words training set\n",
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(train_vector, y_train)\n",
    "\n",
    "# Generating prediction on the vectorized testing set\n",
    "y_pred = naive_bayes_model.predict(test_vector)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy of the naive bayes classifier is: \" + str(round(100 * accuracy, 3)) + \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the SVM classifier is: 41.351%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Training an SVM with linear kernel on the training data\n",
    "# Using a linear kernel to leverage its simplicity and faster training time compared to more fine-tuned SVMs\n",
    "svm_model = SVC(kernel = 'linear')\n",
    "svm_model.fit(train_vector, y_train)\n",
    "\n",
    "# Getting linear SVM predictions on the testing set\n",
    "y_pred = svm_model.predict(test_vector)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy of the SVM classifier is: \" + str(round(100 * accuracy, 3)) + \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] END ..................C=0.01, gamma=0.01, kernel=linear; total time=  47.7s\n",
      "[CV] END ..................C=0.01, gamma=0.01, kernel=linear; total time=  47.6s\n",
      "[CV] END ..................C=0.01, gamma=0.01, kernel=linear; total time=  49.1s\n",
      "[CV] END .....................C=0.01, gamma=0.01, kernel=rbf; total time=  55.5s\n",
      "[CV] END .....................C=0.01, gamma=0.01, kernel=rbf; total time=  54.0s\n",
      "[CV] END .....................C=0.01, gamma=0.01, kernel=rbf; total time=  54.7s\n",
      "[CV] END ...................C=0.01, gamma=0.1, kernel=linear; total time=  48.3s\n",
      "[CV] END ...................C=0.01, gamma=0.1, kernel=linear; total time=  49.5s\n",
      "[CV] END ...................C=0.01, gamma=0.1, kernel=linear; total time=  47.9s\n",
      "[CV] END ......................C=0.01, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END ......................C=0.01, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END ......................C=0.01, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END .....................C=0.01, gamma=1, kernel=linear; total time=  48.3s\n",
      "[CV] END .....................C=0.01, gamma=1, kernel=linear; total time=  48.6s\n",
      "[CV] END .....................C=0.01, gamma=1, kernel=linear; total time=  49.0s\n",
      "[CV] END ........................C=0.01, gamma=1, kernel=rbf; total time=  59.9s\n",
      "[CV] END ........................C=0.01, gamma=1, kernel=rbf; total time= 1.0min\n",
      "[CV] END ........................C=0.01, gamma=1, kernel=rbf; total time= 1.0min\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=  49.0s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=  48.9s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=  48.3s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  55.1s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  55.4s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  54.9s\n",
      "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=  48.1s\n",
      "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=  50.5s\n",
      "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=  48.6s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  48.2s\n",
      "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  48.4s\n",
      "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  48.1s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.0min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.0min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.0min\n",
      "[CV] END .....................C=1, gamma=0.01, kernel=linear; total time= 1.0min\n",
      "[CV] END .....................C=1, gamma=0.01, kernel=linear; total time= 1.1min\n",
      "[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=  58.5s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  54.4s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  55.7s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  55.2s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time= 1.0min\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time= 1.1min\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=  59.7s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time= 1.0min\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time= 1.1min\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=  59.1s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ....................C=10, gamma=0.01, kernel=linear; total time= 1.1min\n",
      "[CV] END ....................C=10, gamma=0.01, kernel=linear; total time=  60.0s\n",
      "[CV] END ....................C=10, gamma=0.01, kernel=linear; total time= 1.0min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 1.3min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 1.2min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 1.2min\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time= 1.1min\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time= 1.0min\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time= 1.1min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 1.6min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 1.6min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 1.6min\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time= 1.0min\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time= 1.0min\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time= 1.0min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 2.1min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 2.1min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 2.1min\n",
      "The accuracy of the SVM(with 3-CV) classifier is: 50.069%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Selecting an array of hyperparameters for hyperparameter tuning using GridSearchCV() function\n",
    "parameters = {\n",
    "  'C' : [0.01, 0.1, 1, 10],\n",
    "  'kernel' : ['linear', 'rbf'],\n",
    "  'gamma' : [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Setting up the hyperparameter tunings\n",
    "svm_cv_model = GridSearchCV( estimator = SVC(),\n",
    "                         param_grid = parameters,\n",
    "                         cv = 3,\n",
    "                         verbose = 2\n",
    ")\n",
    "\n",
    "# Running GridSearchCV() on training set\n",
    "svm_cv_model.fit(train_vector, y_train)\n",
    "\n",
    "# Getting prediction on testing set\n",
    "y_pred = svm_cv_model.predict(test_vector)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy of the SVM(with 3-CV) classifier is: \" + str(round(100 * accuracy, 3)) + \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xenexjoshi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    yasmin miller drove home laundromat englewood ...\n",
       "1    many chicagoans celebrating fourth july barbec...\n",
       "2    july weekend marred wrong kind spate shootings...\n",
       "3    treasury secretary janet yellen tuesday warned...\n",
       "4    treasury secretary janet yellen tuesday told c...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Downloading stopwords form nltk.corpus for pre-processing input_text for CNN\n",
    "nltk.download('stopwords')\n",
    "filter = set(stopwords.words('english'))\n",
    "\n",
    "# Pre-processing function that removes the stopwords, and converts all words to lowercase for feeding into the CNN\n",
    "def preprocess(input, filter):\n",
    "  input = input.lower()\n",
    "  tags = input.split()\n",
    "  tags = [i for i in tags if i.isalpha() and i not in filter]\n",
    "  return \" \".join(tags)\n",
    "\n",
    "# Applying the custom pre-processing function on the input dataset\n",
    "input_text = input_text.apply(lambda x: preprocess(x, filter))\n",
    "input_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 ... 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "# Initializing the LabelEncoder() to encode the labels on the dataset\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Encoding input labels into integers(0, 1, 2)\n",
    "input_label = encoder.fit_transform(input_label)\n",
    "print(input_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ... 14419    12  4215]\n",
      " [    0     0     0 ...   433   230   524]\n",
      " [    0     0     0 ...  4012  1515   120]\n",
      " ...\n",
      " [    0     0     0 ...   904  1212    66]\n",
      " [    0     0     0 ...   558   919   670]\n",
      " [    0     0     0 ...   102    15   782]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max word-count for the CNN tokenizer\n",
    "word_count = 20000\n",
    "\n",
    "# Initializing the Tokenizer, and fitting the input text using the Tokeinizer()\n",
    "tokenizer = Tokenizer(num_words = word_count)\n",
    "tokenizer.fit_on_texts(input_text)\n",
    "\n",
    "# Generating tokenized sequence from tokenizer, and padding sequence to ensure constant length inputs\n",
    "sequences = tokenizer.texts_to_sequences(input_text)\n",
    "input_text = pad_sequences(sequences, maxlen = 200)\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting bias_data.csv into 70-30 training set and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_text, input_label, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting CNN parameters\n",
    "vocab_size = 20000\n",
    "embedding_dim = 128\n",
    "max_length = 200\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_4          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_17 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m20,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_4          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,581,667</span> (9.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,581,667\u001b[0m (9.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,581,667</span> (9.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,581,667\u001b[0m (9.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(input_dim = word_count, output_dim = embedding_dim, input_length = max_length),\n",
    "                            tf.keras.layers.Conv1D(filters = 32, kernel_size = 5, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n",
    "                            tf.keras.layers.GlobalMaxPooling1D(),\n",
    "                            tf.keras.layers.Dense(32, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n",
    "                            tf.keras.layers.Dropout(0.5),\n",
    "                            tf.keras.layers.Dense(3, activation = 'softmax')])\n",
    "\n",
    "model.build((None, max_length))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.4549 - loss: 1.3504 - val_accuracy: 0.4846 - val_loss: 1.0462\n",
      "Epoch 2/5\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.4681 - loss: 1.0497 - val_accuracy: 0.4846 - val_loss: 1.0373\n",
      "Epoch 3/5\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.4773 - loss: 1.0414 - val_accuracy: 0.4846 - val_loss: 1.0366\n",
      "Epoch 4/5\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.4758 - loss: 1.0379 - val_accuracy: 0.4846 - val_loss: 1.0373\n",
      "Epoch 5/5\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.4721 - loss: 1.0357 - val_accuracy: 0.4846 - val_loss: 1.0358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15e070400>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                              patience = 1.4,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train, y_train, epochs = num_epochs, batch_size = batch_size, validation_split = 0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4752 - loss: 1.0402\n",
      "The accuracy of the CNN classifier is: 46.913%.\n"
     ]
    }
   ],
   "source": [
    "_, accuracy= model.evaluate(x_test, y_test)\n",
    "print(\"The accuracy of the CNN classifier is: \" + str(round(100 * accuracy, 3)) + \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>bias_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York Times (News)</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York Post (News)</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Epoch Times</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 source bias_rating\n",
       "0   0  New York Times (News)        left\n",
       "1   1        Chicago Tribune      center\n",
       "2   2   New York Post (News)       right\n",
       "3   3        The Epoch Times       right\n",
       "4   4        Washington Post        left"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_bias = dataset[['id', 'source', 'bias_rating']]\n",
    "input_source = source_bias['source'].astype(str)\n",
    "input_tag = source_bias['bias_rating'].astype(str)\n",
    "source_bias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tags = LabelEncoder()\n",
    "encoder_tags.fit_transform(input_tag)\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(input_source, input_tag, test_size = 0.30, random_state = 42)\n",
    "\n",
    "y_tr = encoder_tags.fit(y_tr)\n",
    "y_te = encoder_tags.fit(y_te)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = \"english\", lowercase = True)\n",
    "train_vector = vectorizer.fit_transform(x_tr)\n",
    "\n",
    "test_vector = vectorizer.transform(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the naive bayes classifier is: 96.415%.\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_source= MultinomialNB()\n",
    "naive_bayes_source.fit(train_vector, y_train)\n",
    "\n",
    "y_pred = naive_bayes_source.predict(test_vector)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy of the naive bayes classifier is: \" + str(round(100 * accuracy, 3)) + \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bias(source):\n",
    "  src = vectorizer.transform([source])\n",
    "  result = naive_bayes_source.predict(src)\n",
    "  print(result)\n",
    "  bias = encoder_tags.inverse_transform(result)[0]\n",
    "  output_str = \"The predicted bias of \" + source + \" is \" + str(bias) + \".\"\n",
    "  return output_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
